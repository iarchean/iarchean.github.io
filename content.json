{"pages":[{"title":"Archean Zhang","text":"System Engineer @ TokyoEmail: zephyr422 @ gmail.com","link":"/about/index.html"}],"posts":[{"title":"苹果 iCloud Keychain 自带的双重认证器好用吗？","text":"首先，双重认证（Two-Factor Authentication or 2FA）是什么？ 双重认证（2FA）是一个安全过程，用户必须提供两种不同的身份验证因素来验证其身份并访问其帐户。此过程确保更好地保护用户的个人信息、凭据和其他资产，同时提高用户可以访问的资源的安全性。(Source: What is 2FA?) 简单来说，双重认证，即「你是谁、你有什么、你知道什么」三者中的两个。 我是一个小白/我的账号不那么重要/谁会来攻击我啊，有必要使用双重认证吗？ 我的观点：任何人都有必要使用双重认证。 安全是永恒的话题，特别是你当今的生活消费娱乐消遣都在互联网上进行，而你用了生日/女友生日/门牌号/名字拼音当作密码时，2FA 就显得尤为重要。而 2FA 实际上是无所不在的：当你登录微信时，会判读是否是常用设备；当你登录淘宝时，会要短信验证码；当你登录银行 App 时，会要扫脸；当你登录 AppleID 时，要用另一台苹果设备确认。那么你的常用设备、你的手机号、你的脸、你的另一台手机，就是第二重身份认证元素：一个攻击者不易获得的「东西」。 既然 App 已经有了 2FA，那我为什么还要使用双重认证器？ 双重认证器为一些不容易采用生物信息、设备绑定，或者没有记录手机号的良心应用（不常出现在简体中文互联网环境中），提供了一个比较通用的解决方案，因此在一些面向消费者的 Web 应用中广泛采用。 说回苹果iOS 15 最令我期待的功能之一，便是苹果自家的双重认证器，它没有单独的应用，是集成在 Keychain 中的；似乎也没有官方命名，我姑且称其为 iCloud Keychain 2FA。2FA 应用有很多，Google、Microsoft、Twilio 等大厂都推出了自己的应用，其实一向注重安全的苹果动作这么慢还蛮罕见的，而且应用目前仍然比较初级。 iCloud Keychain 2FA 也是基于时间的一次性密码 (TOTP) 应用程序，与其他的 2FA 应用差不多：使用移动设备的应用程序扫描 QR 码或手动输入 Setup Key，之后应用程序会显示六位数代码，将其填入网站即可完成绑定。同时网站会提供一系列恢复码（recovery codes），以便在 2FA 应用无法使用时，消耗一枚恢复码，一次性的访问账户，当然，这是网站提供的功能，就与认证器本身无关了。 iCloud Keychain 2FA 与系统的深度集成是我期待它的主要原因，另外其他的认证器在换手机时总有各种各样的问题，让我不得不考虑使用操作系统原生、基于 iCloud 同步的解决方案。 那么，原生的使用体验如何？先看设定（过程不再赘述，GitHub 这篇文章写的足够详细了。） 与其他 TOTP 2FA 应用一样，在网站开启 2FA，选择 Authentication App 时，iCloud Keychain 2FA 也需要扫码。这个过程通过自带相机 App 完成，扫码后选择相应的已经记住的账号即可。 与其他 App 略有差别，iCloud Keychain 2FA 在绑定时步骤稍少，得益于 iOS 的跨平台先天优势，无论是设定还是登录过程中，填入验证码时，均可以自动完成，体验非常棒。 至此设定完成，别忘了保存好自己的 Recover Code。 登录时，体验依然极好 无论是电脑还是手机上的 Safari，输完账号密码后，均可自动填入验证码，以至于，在此章节中我实在没什么可以说的。 iOS 原生 App 我仅仅在有限的几个 App 中测试了 iCloud Keychain 2FA，毕竟这里不是双重认证器的主要战场。但如果你的 App 想要接入，应用中可以直接调起 iCloud Keychain 2FA，省略扫码或者手动输入 Setup Key 的过程，体验比浏览器中还要好很多。 非 Apple 平台上，又会怎样一番光景呢事实上，就算是 Apple 平台上，非 Safari 浏览器的支持也是非常有限，基本没有办法自动填充。而 Apple 把这个功能隐藏得非常深，以至于手动填充验证码的时候，要大费一番周折。只能说无论是 iOS、iPadOS 还是 macOS，此功能都可以工作，仅此而已。 首先是 iOS 和 iPadOS 需要依次进入 Settings → Passwords (FaceID 认证)→ 搜索用户名 → 点击对应网站（如果不同网站使用的相同用户名）→ 点击 VERIFICATION CODE → Copy Verification Code → 粘贴到对应输入框 当然，你也可以抄过去，省略后三步。 macOS 上也好不了哪去 而更过分的是，入口在 Safari 设置中，并不在 Keychain 应用里。 打开 Safari → Preferences → Passwords (输入密码！) → 搜索用户名 → 点击对应网站（如果不同网站使用的相同用户名）→ 点击 VERIFICATION CODE → Copy Verification Code → 粘贴到对应输入框 同样的，也可以省略后三步。 对比其他的双重认证器，iCloud Keychain 2FA 在第三方浏览器上复制验证码的操作之繁琐可谓匪夷所思，也许苹果的工程师根本没有指望你在非苹果生态下使用这个功能，说实在的，像我这种偶尔玩玩游戏的用户，基本没有在 Windows 上登录账号的想法了。 结论就显而易见了优点：在原生 App 和支持的浏览器中，iCloud Keychain 2FA 工作的异常流畅，体验极其丝滑，胜过目前市面上存在的所有 2FA 应用。 缺点：不直观是 iCloud Keychain 2FA 的最大问题。也许苹果不认为 2FA 值得一个单独的 App，或者有个单独的设定入口，就像乔布斯当年说 Dropbox 不应该是应用，而只应该是功能而已。 如果你是苹果全家桶用户，完全可以将所有的 2FA 迁移至 iCloud Keychain，享受苹果生态带来的极致体验；但如果你有任何非苹果设备，在其上你要忍受苹果产品设计中无处不在的傲慢。 References Two Factor Auth | BrainStation® Configuring two-factor authentication - GitHub Docs 所有操作均在 iOS 15.0，iPadOS 15.0，macOS 12.0 完成 我已经完全迁移至 iCloud Keychain 2FA","link":"/2021/12/06/Is-the-2FA-comes-with-Apple-iCloud-Keychain-easy-to-use/"},{"title":"F5 可用性研究以及使用 Zabbix 监控 F5","text":"目前部门的各类系统大部分都使用 F5 发布 VIP 提供给用户使用，而之前对 F5 的监控基本没有，本文诣在研究如何通过 SNMP 的方式读取 F5 的各类状态、性能指标，以及什么样的状况需要去告警。 将 F5 的 OID 模版导入到 Zabbix Server 的 /usr/share/snmp/mibs 目录中之后，即可使用 snmpwalk 命令请求 F5 的各项状态。在请求时，大部分值均需要增加 OID 前缀，本文基本只涉及 F5-BIGIP-LOCAL-MIB.txt 这个模版中的内容，所以请求命令的样子如下： 1snmpwalk -v2c -c public F5.hostname.or.IP F5-BIGIP-LOCAL-MIB::ltmPoolMemberMonitorStatus F5 StatusF5 自身状态中需要关注的一个是 CPU、内存使用率，另外就是主备同步状态及故障转移状态。 Failover Status待补充 ConfigSync Status待补充 Virtual ServerVirtual Server（VS）是 F5 对外提供访问的入口，是 Local Traffic Manager（LTM）中最外层的对象，一旦 Down 掉了整体服务将不可用。这也是除 F5 本身挂掉以外最严重的问题。 VS StatusVS 状态对应的一个 OID 值是 ltmVsStatusAvailState，这代表了 VS 可用状态。除此之外，还需要关注 VS 的运营状态，其 OID 值是 ltmVsStatusEnabledState。 ICON 可用状态 运营状态 可用状态代码 运营状态代码 描述 Avaliable Enabled GREEN（0） ENABLED(1) VS 已启用且池成员健康检查正常 Unknown Enabled BLUE（4） ENABLED(1) VS 已启用但没有成员 Offline Enabled RED（3） ENABLED(1) VS 已启用但池成员健康检查异常 Avaliable Disabled GREEN（0） DISABLED(2) VS 已禁用且池成员健康检查正常 Unknown Disabled BLUE（4） DISABLED(2) VS 已禁用但没有成员 Offline Disabled RED（3） DISABLED(2) VS 已禁用且池成员健康检查异常 RED(3) 状态应该是我们最需要关注的状况。如果运营状态为 Enabled 且可用状态是 Offline，即代表此 VS 出现了不可用的状况。 VS StatisticsVS Statistics 中比较需要关注的包括流量、连接数、请求次数。分别对应下面几个 OID 值： ltmVirtualServStatClientCurConns ltmVirtualServStatTotRequests ltmVirtualServStatClientBytesOut ltmVirtualServStatClientBytesIn PoolPool 是 VS 的资源池，通过某种负载均衡方式将请求转发至池中的 Member。 Pool Status资源池也是有状态的，但只有可用状态，无运营状态。相应的，其 OID 值是 ltmPoolStatusAvailState，池的状态同样列举如下： ICON 可用状态 可用状态代码 描述 Avaliable GREEN（0） 池成员健康检查正常 Unknown BLUE（4） 池成员健康检查状态未知 Offline RED（3） 池成员健康检查异常 关注资源池的状态意义不太明显，因为就目前的服务发布架构（一个 VS 对应一个 Pool），池状态变为 Offline 时，VS 一定也会变成 Offline。所以基本上只需要关注 Pool 的统计数字即可。 Pool StatisticsPool Statistics 中的统计数字可以为整体运营数据提供另一个纬度的指标，意义不如 VS Statistics 那样明显，对应的 OID 值如下： ltmPoolLbMode ltmPoolStatusAvailState ltmPoolActiveMemberCnt ltmPoolStatServerCurConns ltmPoolStatTotRequests ltmPoolStatServerBytesOut ltmPoolStatServerBytesIn NodeNode 是 LTM 中最小的粒度，对应的是实际的服务器（Real Server or RS），Node 是 Pool 中的成员， Node Status这里讨论的 Node 的状态只包括运营状态 ltmNodeAddrStatusEnabledState，因为在现存架构中，Node 的可用状态必须结合 Pool 的健康检查方式来看。Node 可用状态也可以单独设定，但不在本文的讨论范围内。 ICON 运营状态 运营状态代码 描述 黑色 Disabled disabled(1) 已禁用 其他 Enabled enabled(0) 已启用 Node StatisticsNode 的统计数字仅仅作为单机流量的参考，主要包括下面几个 OID 值： ltmNodeAddrStatServerBytesIn ltmNodeAddrStatServerBytesOut ltmNodeAddrStatServerCurConns Pool Member Monitor Status其实相比 Pool Status，更有用的应该是 Pool Member Monitor Status，由于所有的业务均由 F5 统一发布，所以 F5 有着最为敏锐、实时的健康检查机制：一旦发布的服务端口不可用，或延迟较高，则马上将其在 Pool 中排除，以免将用户请求分发到问题节点，从而引起访问超时甚至不可用。 于是，监控 F5 的健康检查状态，比直接使用 Zabbix 的端口检测更加直接。但是，由于 F5 自己会排除问题节点，所以单纯的节点故障不会引起整个服务的异常。此监控产生的告警仅仅需要知会管理员，按照我的监控定义，生成二级告警即可。 ICON 可用状态 可用状态代码 描述 Availabe up(4) 可用 Offline down (19) 不可用 Forced Down forcedDown (20) 已禁用且不可用 Pool Active Member Percentage目前的服务发布机制，每个 Pool 中的服务器数量均基本大于 3 个，相比单纯的 Member Status，关注 Pool 中活动节点的比例显得更为重要。 服务架构设计之初，即考虑到了其冗余性，极端状况下，允许一半的节点停止服务，于是 Active Member Percentage 可以通过活动节点数量 ltmPoolActiveMemberCnt 除以全部节点数量 ltmPoolMemberCnt 来计算出来。 Alarm Policy根据上述内容，我制定出 F5 监控的告警策略，分为一、二、三级告警，分别对应 High、Average、Warning 三个内置告警级别： High VS Offline Pool Active Member Percentage &lt; 50% CPU Usage &gt; 80% Average Pool Member Monitor Status Offline Failover Status Failed Pool Offline Warning Pool Member Monitor Status Disabled ConfigSync Status NotSync Node Disabled VS Disabled","link":"/2019/03/24/Moniting-F5-with-Zabbix/"},{"title":"用Linux做路由器","text":"今日完成了一个专线项目, 根据集团要求, 需要使用 NAT 的方式联通两个办公区之间的内部网络, 使得可以互相访问服务器资源, 并且保存3个月的 NAT 日志, 出问题可以快速定位到人. 由于我们办公区没有路由器设备, 于是采用 Linux 服务器做 iptables 转发来实现 NAT 功能, 并且 iptables 可以记录日志; 此项目设计的客户端和服务器不足百台, 不会有太高的并发访问, 一台中等配置的 Linux 服务器完全可以满足要求. 拓扑结构 我负责的区域在左侧. 办公区和总部通过一条 20M 的 MSTP 线路相连, 互联网段是172.17.176.48/29. 在办公区10.250.0.0/16可以通过 SNAT 方式访问总部网络10.44.0.0/16, 源 IP 需要 NAT 为172.17.176.50/29 在总部可以通过访问互联地址172.17.176.51/29访问办公区的数据库服务器10.250.8.241 基本配置首先在我们10.250.0.0/16这个三层交换机上增加10.44.0.0/16的路由: 1ip route-static 10.44.0.0 255.255.0.0 10.250.254.251 若要实现双向的 NAT, 还有一个必要条件是一台最小化安装的 Linux 服务器, 最少配置2个网卡, 在此场景下, 网卡配置为: eth0作为内部网卡, IP 地址是10.250.254.251; eth1作为互联网卡, IP 地址是172.17.176.50; eth1对端的 IP 为172.17.176.49. 服务器的路由配置, 需要能够访问办公区正常的网络资源, 同时将10.44.0.0/16路由至互联网卡: 1234567# netstat -nrKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface172.17.176.48 0.0.0.0 255.255.255.248 U 0 0 0 eth110.250.254.0 0.0.0.0 255.255.255.0 U 0 0 0 eth010.44.0.0 172.17.176.49 255.255.0.0 UG 0 0 0 eth110.250.0.0 10.250.254.254 255.0.0.0 UG 0 0 0 eth0 开启Linux 内核的转发功能, 编辑/etc/sysctl.conf, 加入如下语句: 1net.ipv4.ip_forward = 1 访问总部网络资源此步骤涉及到 SNAT 的概念. 所谓 SNAT, 即将一段 IP 包的源地址改变, 由于总部的 ACL 策略中仅允许172.17.176.48/29网段访问, 所以我们访问总部时就必须将源地址 NAT 为上述网段. iptables 的 nat tables 有3个内置的 chain, 分别是PREROUTING, POSTROUTING和OUTPUTchain PREROUTING chain – Alters packets before routing. i.e Packet translation happens immediately after the packet comes to the system (and before routing). This helps to translate the destination ip address of the packets to something that matches the routing on the local server. This is used for DNAT (destination NAT).POSTROUTING chain – Alters packets after routing. i.e Packet translation happens when the packets are leaving the system. This helps to translate the source ip address of the packets to something that might match the routing on the desintation server. This is used for SNAT (source NAT).OUTPUT chain – NAT for locally generated packets on the firewall. 我们需要使用的是POSTROUTING chain. 1# iptables -A POSTROUTING -s 10.250.0.0/16 -o eth1 -j SNAT --to-source 172.17.176.50 解释一下, 就是将来自10.250.0.0/16网段的包, 由 eth1 网口出去时的源 IP 变更为172.17.176.50; 我们之前已经将路由配置为访问10.44网段时都走 eth1, 所以此行解决了访问总部的问题. 如果查看, 可以看到此条目已经生效: 123# iptables -L -t nattarget prot opt source destinationNAT all -- 10.250.0.0/16 0.0.0.0/0 to:172.17.176.50 检查生效后可以使用/etc/init.d/iptables save 将此条保存到/etc/sysconfig/iptables 中, 这样重启iptables 后也不会丢是配置. 总部访问办公区服务器此步骤不仅涉及 SNAT, 还涉及到了 DNAT 的概念. DNAT 即将一段 IP 包的目的地址改变. 此方式","link":"/2014/02/07/NAT-on-linux/"},{"title":"使用一台Raspberry Pi作为家庭网关, 无障碍翻墙","text":"Arthor: Archean Zhang Email: zephyr422@gmail.com Version: 1.0.1 Date: 2013/8/16 长期被GFW困扰, 我终于忍不住动手了: 把家里的Raspberry Pi和与自由世界的主机建立OpenVPN over Stunnel的链路, 根据大中华3000条路由来匹配, 国外网站自动走OpenVPN出去以实现翻墙. 以下文章是基于一台Linux服务器(CentOS 6.4)撰写的, 将下面Router的换成Raspberry Pi也一样. 1. 准备硬件: 1台位于自由世界的Linux服务器 (Server) 1台位于本地的Linux服务器 (Router) 客户端 (Client) 软件: Stunnel OpenVPN DNSMasq 拓扑图: 2. Server端配置安装好Linux系统后(Red hat或CentOS), 同步服务器时间(非常重要): 1# ntpdate time.nist.gov 2.1 OpenVPN下载安装OpenVPN, 生成服务器端证书, OpenVPN依赖lzo, 需要一同安装: 12# yum install lzo openvpn 服务器端配置文件/etc/openvpn/server.conf, 范例及说明如下: 1234567891011121314151617local 127.0.0.1 #监听本地接口port 4443 #监听端口proto tcp #协议dev tap ca /etc/openvpn/keys/ca.crt #证书cert /etc/openvpn/keys/server.crt #证书dh /etc/openvpn/keys/dh1024.pem #证书server 172.31.189.0 255.255.255.0 #网段client-to-clientduplicate-cnkeepalive 10 120comp-lzopersist-keypersist-tunstatus openvpn-status.log #状态日志log-append openvpn.log #执行日志verb 3 启动openvpn: 1# openvpn --config /etc/openvpn/server.conf --deamon 观察日志, 如果出现“Initialization Sequence Completed”则代表启动成功. 2.2 StunnelOpenvpn的Server端只监控本地接口, 就是为了用Stunnel将流量加密, 在Local端安全的链接, 达到加密, 混淆流量的作用, 以绕开GFW的监控. 下载安装stunnel 12# yum install stunnel 安装过程中会生成服务器证书stunnel.pem, 按照提示来即可. 编辑配置文件/usr/bin/etc/stunnel.conf, 如下所示: 1234567891011121314151617181920212223cert = /usr/local/etc/stunnel/stunnel.pemCAfile = /usr/local/etc/stunnel/stunnel.pemsocket = l:TCP_NODELAY=1socket = r:TCP_NODELAY=1pid = /tmp/stunnel.pidverify = 3setuid = stunnelsetgid = stunnelcompression = zlibdelay = nosslVersion = TLSv1fips=nodebug = 7syslog = nooutput = /usr/local/etc/stunnel/stunnel.log[s-openvpn] accept = 13579 #监听端口connect = 127.0.0.1:4443 #OpenVPN端口 启动stunnel: 1# stunnel 查看OpenVPN和Stunnel是否正确执行, 检查相应端口: 1234567891011# netstat -ntwlaActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:13579 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:4443 127.0.0.1:50223 ESTABLISHED # lsof -i:4443 -nCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEopenvpn 25290 root 5u IPv4 2982393 0t0 TCP *:pharos (LISTEN)openvpn 25290 root 8u IPv4 3361561 0t0 TCP 127.0.0.1:pharos-&gt;127.0.0.1:50223 (ESTABLISHED)stunnel 26801 stunnel 10u IPv4 3361560 0t0 TCP 127.0.0.1:50223-&gt;127.0.0.1:pharos (ESTABLISHED) 看到端口均已正确监听, 至此Server端配置完成 3. Router配置安装好Linux系统后(Red hat或CentOS), 同步服务器时间(非常重要): 12# ntpdate time.nist.gov 3.1 安装StunnelRouter端需要首先安装Stunnel, 在本地开启加密链路, 透传OpenVPN加密端口4443, 然后OpenVPN才能开始接入: 1# yum install stunnel 将Server端Stunnel证书传过来, 以便验证: 1# scp root@server:/usr/local/etc/stunnel.pem root@router:/usr/local/etc/stunnel.pem 开始进行stunnel配置, 配置文件/usr/local/etc/stunnel.conf: 1234567891011121314151617pid = /tmp/stunnel.pidcert = /usr/local/etc/stunnel/stunnel.pemsocket = l:TCP_NODELAY=1socket = r:TCP_NODELAY=1verify = 3CAfile = /usr/local/etc/stunnel/stunnel.pemclient=yescompression = zlibciphers = AES256-SHAdelay = nofailover = priosslVersion = TLSv1output = /root/bin/logs/stunnel.log[s-openvpn]accept = 127.0.0.1:4443connect = server.ip.address:13579 启动stunnel: 1# stunnel 3.2 安装OpenVPN下载安装OpenVPN, OpenVPN依赖lzo, 需要一同安装: 1# yum install lzo openvpn 配置Router端配置文件/etc/openvpn/hk.ovpn: 123456789101112131415161718dev tap #设备模式port 65530 #本地监听端口proto tcp #协议client #服务模式:clienttls-client #加密客户端ns-cert-type server remote 127.0.0.1 4443 #Server端口, 由于使用Stunnel加密透传, 所以连接本地端口ca /etc/openvpn/ca/ca.crt #证书key /etc/openvpn/ca/client1.key #证书cert /etc/openvpn/ca/client1.crt #证书persist-keypersist-tun#route-method exe#route-delay 2comp-lzostatus /etc/openvpn/openvpn-status.log #状态日志log-append /etc/openvpn/ca.log #执行日志verb 3 启动OpenVPN: 1# openvpn --daemon --config /etc/openvpn/hk.ovpn 观察日志, 如果出现“Initialization Sequence Completed”则代表启动成功. 检查隧道: 123456789101112131415# ip addr11: tap0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 100 link/ether 06:10:50:e5:e5:2c brd ff:ff:ff:ff:ff:ff inet 172.31.188.2/24 brd 172.31.188.255 scope global tap0 inet6 fe80::410:50ff:fee5:e52c/64 scope link valid_lft forever preferred_lft forever # ping 172.31.188.1PING 172.31.188.1 (172.31.188.1) 56(84) bytes of data.64 bytes from 172.31.188.1: icmp_seq=1 ttl=64 time=67.3 ms64 bytes from 172.31.188.1: icmp_seq=2 ttl=64 time=67.1 ms^C--- 172.31.188.1 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1588msrtt min/avg/max/mdev = 67.183/67.284/67.386/0.278 ms 链路通畅 3.3 安装DNSmasqRouter若要进行翻墙, 需要使用国外DNS, 而浏览国内网站时, 如果使用国外DNS, 会造成将访问导向国外站点, 造成访问速度变慢, 所以此方案使用DNSmasq解决此问题. 同时DNSmasq也是一个轻量级DHCP服务器, 非常方便好用. 安装DNSmasq: 1# yum install dnsmasq 编辑DNSmasq配置文件/etc/dnsmasq.conf, 假设Router本地IP是10.2.166.10, 本地分发网段是10.2.166.0/24, 则配置如下: 12345678910111213141516171819202122# DHCP configexpand-hostsdomain=archean.me# DHCP Rangedhcp-range=10.2.166.50,10.2.166.150,12h #DHCP地址池# DHCP routedhcp-option=3,10.2.166.10 #路由# Apple ntp serverdhcp-option=option:ntp-server,10.3.1.233 #ntp时间服务器# DNS serverno-resolvno-pollserver=8.8.8.8server=8.8.4.4conf-dir=/etc/dnsmasq.d# AppleTV trailersaddress=/trailers.apple.com/180.153.225.136# Static IPs# dhcp-host=xx:xx:xx:xx:xx:xx,10.2.166.121 配置分类解析配置文件/etc/dnsmasq.d/china.conf, 以使常用域名走国内DNS解析, 举例如下, 根据自身情况更改: 1234567891011# server=/domain.name/dns.serverserver=/115.com/114.114.114.114server=/123u.com/114.114.114.114server=/126.com/114.114.114.114server=/126.net/114.114.114.114server=/163.com/114.114.114.114server=/17173.com/114.114.114.114server=/17cdn.com/114.114.114.114server=/51.la/114.114.114.114server=/6rooms.com/114.114.114.114server=/91.com/114.114.114.114 至此准备工作完成, 可以启动路由器了 3.4 启动路由器思路 默认路由为OpenVPN Server端私网IP, chnroute生成的3000条国内路由走国内链路, 以达到分流/翻墙的目的. 步骤: 启动Stunnel, 启动DNSmasq 开启linux内核转发功能 启动OpenVPN 增加3000条国内路由(点这里下载) 增加默认路由, 增加下一跳为OpenVPN Server端私网IP 将步骤编写成脚本, 如下: 1234567891011121314151617181920212223#!/bin/sh# Stunnel up/usr/bin/stunnel/usr/sbin/dnsmasq# date ntp timentpdate ntp.server.name/sbin/route add -net 10.0.0.0/8 gw 10.2.166.1/sbin/route add -host server.ip.address gw 10.2.166.1# iptables NAT/sbin/iptables -t nat -A POSTROUTING -o tap0 -j MASQUERADEsysctl -w net.ipv4.ip_forward=1# start Openvpnkillall openvpn/usr/local/sbin/openvpn --daemon --config /etc/openvpn/hk.ovpnsleep 5# Chnroutersfor i in `cat /root/bin/CN` ; do /sbin/route add -net $i gw 10.2.166.1 ; done/sbin/route del default/sbin/route add default gw 10.2.166.1/sbin/route add default gw 172.31.188.1 启动脚本, Router环境搭建完成, 国外网络访问测试: 1234567891011# tracert 8.8.8.8traceroute to 8.8.8.8 (8.8.8.8), 30 hops max, 60 byte packets 1 172.31.188.1 (172.31.188.1) 68.963 ms 68.913 ms 68.914 ms 2 103.30.4.1 (103.30.4.1) 109.650 ms 109.651 ms 109.645 ms 3 172.16.0.2 (172.16.0.2) 109.410 ms 109.454 ms 109.448 ms 4 gi1-26.br02.hkg04.pccwbtn.net (63.218.241.1) 109.384 ms 109.384 ms 109.465 ms 5 72.14.196.197 (72.14.196.197) 109.415 ms 109.381 ms 109.344 ms 6 209.85.241.56 (209.85.241.56) 109.368 ms 209.85.241.58 (209.85.241.58) 105.488 ms 149.385 ms 7 216.239.43.17 (216.239.43.17) 108.792 ms 68.841 ms 209.85.253.69 (209.85.253.69) 69.430 ms 8 * * * 9 google-public-dns-a.google.com (8.8.8.8) 108.102 ms 107.936 ms 107.819 ms 可以看到直接走OpenVPN私网IP出去, 国内网络访问测试: 1234567891011121314# traceroute www.baidu.comtraceroute to www.baidu.com (115.239.210.26), 30 hops max, 60 byte packets 1 10.2.166.1 (10.2.166.1) 2.267 ms 2.470 ms 2.623 ms 2 10.2.255.254 (10.2.255.254) 2.180 ms 2.492 ms 2.648 ms 3 * * * 4 * * * 5 * * * 6 123.125.40.254 (123.125.40.254) 2.714 ms 2.634 ms 2.883 ms 7 61.49.44.65 (61.49.44.65) 1.459 ms 1.448 ms 1.521 ms 8 61.148.160.5 (61.148.160.5) 1.443 ms 1.466 ms 1.459 ms 9 124.65.60.77 (124.65.60.77) 3.258 ms 5.805 ms 5.790 ms 10 123.126.0.85 (123.126.0.85) 3.392 ms 3.202 ms 3.245 ms11 123.126.0.85 (123.126.0.85) 3.214 ms 5.487 ms 3.913 ms12 219.158.35.90 (219.158.35.90) 71.347 ms 71.338 ms 71.337 ms 国内链路则直接走原本的网络环境出去. 3.5 配置监控脚本由于国际出口不一定稳定, OpenVPN有可能会间歇性断掉, 如果发生此情况, 则会造成默认路由不可达, 从而导致国内网站浏览也会出现问题, 所以配置监控脚本, 一旦国际链路抽风, 则立刻将路由切换至国内. 脚本内容如下: 1234567891011121314151617181920212223#!/bin/bash ip=&quot;172.31.188.1&quot; i=-1 j=-1 while : ; do ping $ip -c 5 -w 30 if [ $? -ge &quot;1&quot; ]; then route del default gw $ip j=-1 i=$((i+1)) txt=`date +%F&quot; &quot;%X`&quot; Ping $ip timed out.&quot; (($i%30==0)) &amp;&amp; echo $txt &gt;&gt; /root/bin/logs/checkip.log (($i%30==0)) &amp;&amp; echo $txt | mailx -s &quot;VPN disconnected&quot; admin@domain.com else route add default gw $ip i=-1 j=$((j+1)) txt=`date +%F&quot; &quot;%X`&quot; Ping $ip OK.&quot; (($j%30==0)) &amp;&amp; echo $txt &gt;&gt; /root/bin/logs/checkip.log (($j%100000==0)) &amp;&amp; echo $txt | mailx -s &quot;VPN OK.&quot; admin@domain.com fi sleep 10 done 后台启动此脚本, 则会每10秒监控一次出国链路, 如果断掉, 将会把路由切至国内, 不影响正常上网; 如果出国链路长期无法恢复, 则每5分钟给管理员发送一封告警邮件. 4. Client客户端接入路由器所在交换机后, 配置为自动获取IP地址, 即可进行翻墙.","link":"/2013/10/16/breaking-gfw-with-raspberry-pi/"},{"title":"使用 Terrafrom 半自动化白嫖 GCP、AWS 搭建翻墙服务","text":"最近公司云主机管理业务统一切换到了 Terraform 这个非常棒的 IaC 工具上，它可以大大简化整个集群管理的难度，经过改造，也实现了统一的配置中心，结合公司现有 Git 服务和流水线工具，让 OP 协同更加高效。 相比 Ansible（两者实际上并不能完全放在一起对比），Terraform 不需要在目标服务器上部署 agent，侵入性较小；配置的是一个最终状态而不是过程；使用也更加简单。我司一般在物理机上使用 Ansible。 所以我觉得有必要向大家推荐一下这个工具，计划写成系列文章，由简入深的介绍几个使用场景。公司中的架构和使用案例不便分享，分享几个大家日常生活中都会遇到的。 首先当然是绕过可恶的 GFW。 本文所有的配置文件均可在我的这个 repo 中找到 GCP、AWS 海外节点的网络质量和路由都比较优质，多年来我一直用他们作为主力梯子使用，另外还在上面部署了很多其他服务，如监控探针、博客后台、测速节点、定时任务、爬虫、推送服务、一些自用 api 等。况且，两者都提供 1 年的免费试用额度。 但是每年都要重新部署一次应用，略显繁琐（Update：大概 2020 年 9 月前后， GCP 的免费试用期限由 1 年缩短为 3 个月了），于是我写了一段 Shell 脚本来批量初始化服务器和应用。今天我们把它改造成 Terraform 工程。 主要流程 在 AWS 或 GCP 上申请一个免费试用账户，获取 AccessKey 和 SecretKey； 然后使用 Terraform 的 AWS/GCP Provider 创建主机实例； 可选：通过 Terraform 的 Output 模块返回的主机 IP 信息，更新 Cloudflare DNS 记录； 安装 Docker； 使用 remote-exec provider 在 Docker 里部署应用，返回关键信息。 以上流程中，除了第 1 步不可避免的要手工操作以外，2-4 仅仅需要2条命令即可实现： 12terraform initterraform apply 环境准备 terraform cli aws cli / gcloud cli 公、私钥对（不能用加密的） 安装 Terraform CLI：macOS 安装 terraform cli 非常容易: 12brew tap hashicorp/tapbrew install hashicorp/tap/terraform 检查是否已经安装好： 1terraform -help 检查公私钥对假设你电脑上已经存在公私钥对（~/.ssh/id_rsa, ~/.ssh/id_rsa.pub），否则的话，参照此文档生成一对。 了解 Terraform 配置语言terraform 配置中有几种常见的语言： Provider Provider 是 Terraform 与云服务商、SaaS 提供商或 API 交互的桥梁，我们今天主要会使用到 2 个 aws（或 gcp） cloudflare Variable 和 Output Variables 和 Output 一个负责输入，一个负责输出，很容易理解。我们的参数会放在 variable 里，例如 accsess token 和 region 等信息。程序执行后需要的信息会放在 output 中，供下一步使用，如 public_ip、elb_ip 等。 Resource resource 是 Terraform 语言中最重要的元素。每个 resource block 描述一个或多个基础设施对象，如虚拟网络、计算实例或更高级别的组件，如 DNS 记录。 State Terraform必须存储有关托管基础设施和配置的状态。Terraform使用此状态将现实世界资源映射到配置中。 编辑配置文件遵照最佳实践，terraform 有几个主要的配置文件需要首先定义好： versions.tf 定义 Prividers 版本 123456789101112terraform { required_providers { aws = { source = &quot;hashicorp/aws&quot; } cloudflare = { source = &quot;cloudflare/cloudflare&quot; version = &quot;3.18.0&quot; } } required_version = &quot;&gt;= 0.13&quot;} outputs.tf 定义输出信息 123output &quot;ipmaster&quot; { value = &quot;aws_instance.app.public_ip&quot;} Key-pairs.tf 定义公私钥对资源配置 1234resource &quot;aws_key_pair&quot; &quot;deployer&quot; { key_name = &quot;deploy-${var.workspace}&quot; public_key = &quot;${file(&quot;~/.ssh/id_rsa.pub&quot;)}&quot;} variables.tf 定义各类输入数据的默认值 123456789101112131415161718192021222324252627variable &quot;access_key&quot; { default = &quot;****&quot;}variable &quot;secret_key&quot; { default = &quot;*****&quot;}variable &quot;cloudflare_apikey&quot; { default = &quot;*****&quot;}variable &quot;cloudflare_zone_id&quot; { default = &quot;*****&quot;}variable &quot;region&quot; { default = &quot;ap-northeast-1&quot;}variable &quot;workspace&quot; { default = &quot;user&quot;}variable &quot;bucket&quot; { default = &quot;bucket&quot;}variable &quot;aws_type&quot; { default = &quot;t2.micro&quot;}variable &quot;aws_ami&quot; { default = &quot;ami-0e00e89380cb2a63b&quot;} 准备工作就绪，下面定义 resource，也就是整个流程中真正执行业务的配置，我们总共有 3 个 resource：分别是创建 ec2 实例的 app-instances.tf，创建防火墙配置的 security_group.tf，和创建 DNS A 记录的 cloudflare.tf。 app-instances.tf 12345678910111213141516171819202122232425provider &quot;aws&quot; { access_key = &quot;${var.access_key}&quot; secret_key = &quot;${var.secret_key}&quot; region = &quot;${var.region}&quot;}resource &quot;aws_instance&quot; &quot;app&quot; { ami = &quot;${var.aws_ami}&quot; instance_type = &quot;${var.aws_type}&quot; security_groups = [&quot;${aws_security_group.app.name}&quot;] key_name = &quot;${aws_key_pair.deployer.key_name}&quot; connection { host = self.public_ip user = &quot;ubuntu&quot; private_key = &quot;${file(&quot;~/.ssh/id_ed25519&quot;)}&quot; } provisioner &quot;remote-exec&quot; { inline = [ &quot;sudo apt update&quot;, ] } tags = { Name = &quot;${var.workspace}-primary&quot; }} security_group.tf 定义 aws 防火墙资源配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445/* Default security group */resource &quot;aws_security_group&quot; &quot;app&quot; { name = &quot;app-group-${var.workspace}&quot; description = &quot;Default security group that allows inbound and outbound traffic from all instances in the VPC&quot; ingress { from_port = &quot;0&quot; to_port = &quot;0&quot; protocol = &quot;-1&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] self = true } ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } ingress { from_port = 21000 to_port = 21000 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } egress { from_port = &quot;0&quot; to_port = &quot;0&quot; protocol = &quot;-1&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] self = true } egress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] } egress { from_port = 21000 to_port = 21000 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] }} cloudflare.tf 12345678910111213141516provider &quot;cloudflare&quot; { email = &quot;zephyr422@gmail.com&quot; api_token = &quot;${var.cloudflare_apitoken}&quot;}resource &quot;cloudflare_record&quot; &quot;aws-test&quot; { zone_id = &quot;${var.cloudflare_zone_id}&quot; name = &quot;aws-test&quot; value = &quot;${aws_instance.app.public_ip}&quot; type = &quot;A&quot; proxied = false depends_on = [ aws_instance.app ]} OK，现在所有的配置均已经完成，让我们 review 一下目录结构： 12345678app/├── app-instances.tf # aws ec2 resource├── cloudflare.tf # dns A record resource├── key-pairs.tf├── outputs.tf├── security-group.tf # firewall recource├── variables.tf└── versions.tf 执行初始化1terraform init 如果一切正常，我们会看到如下返回（删除了多余的描述信息） 1234567891011Initializing the backend...Initializing provider plugins...- Finding latest version of hashicorp/aws...- Finding cloudflare/cloudflare versions matching &quot;~&gt; 3.0&quot;...- Installing hashicorp/aws v4.22.0...- Installed hashicorp/aws v4.22.0 (signed by HashiCorp)- Installing cloudflare/cloudflare v3.18.0...- Installed cloudflare/cloudflare v3.18.0 (signed by a HashiCorp partner, key ID DE413CEC881C3283)Terraform has been successfully initialized! 执行部署1terraform apply terraform 会列出此次部署的所有变化，非常精细，里面有几个值得我们关注的点： 下面的文字是说我们将要执行的，是一个创建任务。 12Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create 总共有 4 个新增，0 个变化，0 个销毁。 1Plan: 4 to add, 0 to change, 0 to destroy. 那具体是哪 4 个新增呢？ 1234# aws_instance.app will be created# aws_key_pair.deployer will be created# aws_security_group.app will be created# cloudflare_record.aws-test will be created 这行表示了执行后的输出都有什么 12Changes to Outputs: + ipmaster = (known after apply) 现在我们充分理解了本次部署的所有操作，输入 yes 确认开始执行。观察一下输出，IP 地址我就不隐藏了反正待会儿就删掉了。 12345678910111213141516171819202122232425262728293031aws_key_pair.deployer: Creating...aws_security_group.app: Creating...aws_key_pair.deployer: Creation complete after 0s [id=deploy-user]aws_security_group.app: Creation complete after 2s [id=sg-0847b070b1c0f3057]aws_instance.app: Creating...aws_instance.app: Still creating... [10s elapsed]aws_instance.app: Provisioning with 'remote-exec'...aws_instance.app (remote-exec): Connecting to remote host via SSH...aws_instance.app (remote-exec): Host: 54.250.229.47aws_instance.app (remote-exec): User: ubuntuaws_instance.app (remote-exec): Password: falseaws_instance.app (remote-exec): Private key: trueaws_instance.app (remote-exec): Certificate: falseaws_instance.app (remote-exec): SSH Agent: falseaws_instance.app (remote-exec): Checking Host Key: falseaws_instance.app (remote-exec): Target Platform: unixaws_instance.app (remote-exec): Connected!...aws_instance.app (remote-exec): psk = f9820fc282341036f21e0f3f46a094faaws_instance.app (remote-exec): obfs = http...aws_instance.app: Creation complete after 1m1s [id=i-038a9d00fab6abe90]cloudflare_record.aws-test: Creating...cloudflare_record.aws-test: Creation complete after 0s [id=7078b55ccea7670a65eefe16f160b074]Apply complete! Resources: 4 added, 0 changed, 0 destroyed.Outputs:ipmaster = &quot;54.250.229.47&quot; 部署执行成功，分别去 AWS Console 和 Cloudflare Dashboard 中查看，资源都按照计划创建完成了 部署的输出中也已经把 psk 打印出来了，剩下的就是放到翻墙代理中使用即可。 另外，terraform apply 命令也支持使用环境变量传递参数，不想将 APIkey 等用明文放入配置文件的话，也可以使用这种方式进行。 1234export TF_VAR_access_key=xxxxexport TF_VAR_secret_key=xxxxxxxxterraform apply 从今往后，只要每年创建一个新的 AWS（GCP）账号，执行一下 terraform apply，就可以马上恢复你的云主机配置。","link":"/2018/05/07/deploy-app-to-aws-and-gcp-with-terraform/"},{"title":"Docker 学习笔记","text":"最近公司的一些服务将要迁移至 Docker 平台，为了能够理解 Docker 原理，决定自己从零开始学习 Docker 这个风靡全球的容器解决方案，将学习的过程记录下来。 安装Docker 在 CentOS 6.10 上的安装公司的系统使用的是 CentOS 6，所以先将内核升级至 3.10 以上版本 123rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgyum install https://www.elrepo.org/elrepo-release-6-8.el6.elrepo.noarch.rpmyum --enablerepo=elrepo-kernel install kernel-lt -y 之后使用下面的命令将 Docker 源加入到 yum 源中 12345678tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF 之后运行下面的命令安装、启动 Docker 123yum install -y docker-engineservice docker start 中间遇到很多问题，大概列举一下 docker: relocation error: docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference 这个是说 device-mapper 版本过低，升级一下 1yum update device-mapper FATAL: Module bridge not found. 内核版本问题，升级到 3.10 以上解决 FATA[0000] Error starting daemon: Error initializing network controller: Error creating default “bridge” network: can’t find an address range for interface “docker0” 此错误是因为公司内网有 172.16.0.0/16 的路由表，将 docker0 的网卡网段占用了导致的，通过在 /etc/sysconfig/docker 中增加以下参数，强制让 Docker 创建虚拟机时使用特定网段来解决： 1other_args=&quot;-bip=192.168.100.1/24&quot; 拉取镜像提示 Get https://index.docker.io/v1/repositories/library/hello-world/images: dial tcp 52.207.42.240:443: network is unreachable 这个肯定是网络问题了，公司的标准服务器是没有办法上外网的，系统环境变量中指定我自己的代理服务器居然也不管用，尝试了多种方法，目前问题还没解决 Docker 在 Ubuntu 16.04 上的安装由于 Docker 最早就是在 Ubuntu 上发展出来的，这里也记录下 Ubuntu 16.04 的安装方法 首先，安装 Docker 官方仓库的 GPG key 到系统中 1curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 添加 Docker apt 源 1sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; 然后，进行一下更新 1sudo apt-get update 确认 Docker repo 可以安装 1apt-cache policy docker-ce 如果看到下面的内容说明是 OK 的 1234567Output of apt-cache policy docker-cedocker-ce: Installed: (none) Candidate: 18.06.1~ce~3-0~ubuntu Version table: 18.06.1~ce~3-0~ubuntu 500 500 https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages 安装 Docker 1sudo apt-get install -y docker-ce 确认 Docker 是否已经安装 1sudo systemctl status docker 类似下面的输出说明正确安装了 12345678● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-10-18 20:28:23 UTC; 35s ago Docs: https://docs.docker.com Main PID: 13412 (dockerd) CGroup: /system.slice/docker.service ├─13412 /usr/bin/dockerd -H fd:// └─13421 docker-containerd --config /var/run/docker/containerd/containerd.toml 另外，docker run 仍然是提示网络问题无法访问，跟 CentOS 一样无论如何加代理都不行 1234# docker run hello-worldUnable to find image 'hello-world:latest' locallydocker: Error response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers).See 'docker run --help'. 最后，只能让 Docker Host 直接连接外网来解决这个问题。 使用执行 Docker 命令开始了解 Docker 命令，最简单的方法就是执行 Hello World 了 1$ docker run hello-world 成功后会看到如下的输出： 1234Hello from Docker!This message shows that your installation appears to be working correctly.... 运行一个 Docker 容器hello-world 这个镜像的作用是输出一段文本然后退出，每一个 Docker 容器其实都是一个有特定功能的应用，比如 ubuntu 这个镜像，就是一个 Ubuntu 虚拟机。 试着运行一个 123$ docker run -it -d ubuntu# -it 参数可以让我们交互式的 shell 进入容器 这个时候会输出一段字符串，这段字符串可以理解为 Docker 容器 ID 141b162b9dd3f4dc2b7d37bf195a2c4f09d38a05cf750cfbf07e90313da2a3995 这个时候，我们就可以进入这个容器内，去一看究竟了 12345678$ docker exec -it 41b16 /bin/bashroot@41b162b9dd3f:/# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 07:09 pts/0 00:00:00 /bin/bashroot 10 0 0 07:11 pts/1 00:00:00 /bin/bashroot 21 10 0 07:13 pts/1 00:00:00 ps -ef 管理用一段时间的 Docker 之后，我们可能会有很多运行中的和非运行状态的容器，怎么去管理它们呢？ 查看 Docker 容器docker ps 命令可以让我们看到目前运行中的 Docker 容器情况 1$ docker ps 输出如下： 123456CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES41b162b9dd3f ubuntu &quot;/bin/bash&quot; 5 minutes ago Up 5 minutes zealous_murdock47651cfa1d8e golang &quot;bash&quot; 14 hours ago Up 14 hours optimistic_lederberg3b80522e5902 busybox &quot;sh&quot; 37 hours ago Up 37 hours zealous_darwin8b881720ceb8 redis &quot;docker-entrypoint.s…&quot; 37 hours ago Up 37 hours 0.0.0.0:6379-&gt;6379/tcp musing_noycef3f14f20b61f nginx &quot;nginx -g 'daemon of…&quot; 37 hours ago Up 37 hours 80/tcp zealous_blackburn 奇怪，我们刚才运行的 Hello World 没有在列表里，什么原因呢？ 其实 Hello World 属于非活动状态的镜像，只要给 docker ps 命令加上 -a 参数就可以看到了 12345678910111213141516$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES41b162b9dd3f ubuntu &quot;/bin/bash&quot; 6 minutes ago Up 6 minutes zealous_murdock258796d0e4ed hello-world &quot;/hello&quot; 12 minutes ago Exited (0) 12 minutes ago trusting_swartz47651cfa1d8e golang &quot;bash&quot; 14 hours ago Up 14 hours optimistic_lederberg3b80522e5902 busybox &quot;sh&quot; 37 hours ago Up 37 hours zealous_darwin8b881720ceb8 redis &quot;docker-entrypoint.s…&quot; 37 hours ago Up 37 hours 0.0.0.0:6379-&gt;6379/tcp musing_noyce0a02170a956f hello-world &quot;/hello&quot; 37 hours ago Exited (0) 37 hours ago determined_mestorff3f14f20b61f nginx &quot;nginx -g 'daemon of…&quot; 37 hours ago Up 37 hours 80/tcp zealous_blackburnb43d4f2ba5ed ubuntu &quot;/bin/bash&quot; 37 hours ago Exited (0) 7 minutes ago unruffled_snyder66aa497d9767 ubuntu &quot;/bin/bash&quot; 37 hours ago Exited (0) 37 hours ago hungry_boyd126e0f019b19 ubuntu &quot;/bin/bash&quot; 37 hours ago Exited (0) 37 hours ago competent_goldwasser600adf161623 ubuntu &quot;/bin/bash&quot; 37 hours ago Exited (0) 37 hours ago upbeat_thompson3b5d4380392f ubuntu &quot;/bin/bash&quot; 37 hours ago Exited (0) 37 hours ago pedantic_euclid4fd577a1e8bf ubuntu &quot;/bin/bash&quot; 37 hours ago Exited (0) 37 hours ago upbeat_mcnulty 而如果使用 -l 参数，则可以看到最近创建的 Docker 容器 1234$ docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES41b162b9dd3f ubuntu &quot;/bin/bash&quot; 10 minutes ago Up 10 minutes zealous_murdock 关闭一个 Docker 容器我们在测试过程中运行了很多容器实例了，一些容器不需要再去使用了， 使用 docker stop 命令可以将活动状态的容器停止运行 1$ docker stop 41b162b9dd3f 不仅可以使用 container ID 指定容器，还可以使用容器别名来操作 123$ docker stop zealous_murdockzealous_murdock 停止运行的容器就可以关掉了，这里使用的命令是 docker rm 123# docker rm zealous_murdockzealous_murdock overlay2 代替 auFS 作为新的 Union File System 来使用","link":"/2016/03/03/docker-learn-memo-1/"},{"title":"为nginx配置https并自签名证书","text":"为了使网站可以使用加密的方式访问, 我们有时需要配置https, 此文章简单介绍一下方法.##把证书准备好。 ###制作CA证书： ca.key CA私钥： openssl genrsa -des3 -out ca.key 2048 制作解密后的CA私钥（一般无此必要）： openssl rsa -in ca.key -out ca_decrypted.key ca.crt CA根证书（公钥）： openssl req -new -x509 -days 7305 -key ca.key -out ca.crt ###制作生成网站的证书并用CA签名认证在这里，假设网站域名为blog.archean.me 生成blog.archean.me证书私钥： openssl genrsa -des3 -out blog.archean.me.pem 1024 制作解密后的blog.archean.me证书私钥： openssl rsa -in blog.archean.me.pem -out blog.archean.me.key 生成签名请求： openssl req -new -key blog.archean.me.pem -out blog.archean.me.csr 在common name中填入网站域名，如blog.archean.me即可生成改站点的证书，同时也可以使用泛域名如*.archean.me来生成所有二级域名可用的网站证书。 用CA进行签名： openssl ca -policy policy_anything -days 1460 -cert ca.crt -keyfile ca.key -in blog.archean.me.csr -out blog.archean.me.crt 其中，policy参数允许签名的CA和网站证书可以有不同的国家、地名等信息，days参数则是签名时限。如果在执行签名命令时，出现I am unable to access the ../../CA/newcerts directory 修改/etc/pki/tls/openssl.cnf中“dir = ./CA” 然后： mkdir -p CA/newcerts touch CA/index.txt touch CA/serial echo “01″ &gt; CA/serial 再重新执行签名命令。 最后，把ca.crt的内容粘贴到blog.archean.me.crt后面。这个比较重要！因为不这样做，可能会有某些浏览器不支持。好了，现在https需要到的网站私钥blog.archean.me.key和网站证书blog.archean.me.crt都准备完毕。接下来开始配置服务端。 ##配置nginx新开一个虚拟主机，并在server{}段中设置： listen 443; ssl on; ssl_certificate /path/to/blog.archean.me.crt; ssl_certificate_key /path/to/blog.archean.me.key; 其中的路径是刚刚生成的网站证书的路径。 然后使用一下命令检测配置和重新加载nginx： 检测配置： nginx -t 重新加载： nginx -s reload 在这里是nginx官方的关于https的文档，可以作为参考。","link":"/2013/10/16/enable-nginx-ssl/"},{"title":"用 Dropbox public folder 做博客的超稳定图床","text":"由于 Dropbox 修改了 Public Folder 的功能，方法已失效。 这个Blog是搭建在Digital Ocean上的, 这个VPS还是很不错的, 加州的节点在天朝访问速度非常可以, 拥有20G SSD磁盘和每月1T的流量, 只需5$, 还是非常超值的. 但是即便如此, Blog需要的大量图片等元素还是非常占用空间的, 所以只好想办法寻找一个稳定的图床来存. 我一直是Dropbox的重度用户, 自然想到她的Public Folder. Dropbox Public目录每日限制流量 20G, 来源 Dropbox Public Folder的地址是https://dl.dropboxusercontent/u/idxxxxxxxx/, Blog引用起来不够优雅, 于是萌生了此文. 配置起来非常简单, 经过我的反复实践, Nginx vhost最小配置为: server { listen 80; server_name img.archean.me; root /path/to/www/root; #也可省略 location / { rewrite /(.*) /u/DropboxID/img/$1 break; proxy_pass https://dl.dropboxusercontent.com/; } } 此配置足以使用, 最终效果是当我访问http://img.archean.me/1.jpg时, 可以显示https://dl.dropboxusercontent.com/u/19060928/img/1.jpg的图片","link":"/2013/10/21/make-dropbox-public-folder-a-stable-pic-server/"},{"title":"使用 Zabbix 监控 Exchange Server 的数据库","text":"在这篇文章里我想要分享一下 Exchange Server 2016 数据库状态以及数据库使用量的监控方法，经过几次修改和迭代，目前我使用的是 Zabbix 3.2 对数据库进行统一、自动化、动态自配置的监控。 主要解决了哪些问题？当前主流的监控平台，如 Zabbix、Prometheus，微软自家的 SCOM，还有古董 Cacti 我都已经进行过或多或少的尝试，最终选定 Zabbix 作为 Exchange Server 的统一监控平台，主要原因是它高自由度的配置、分布式部署方式以及丰富的 API 接口，便于针对性的修改和获取数据用于统一展示，图表功能一般但是够用，好在可以通过 API 将数据取出来做二次处理。 在花费了大量时间优化调整监控项、触发器以及告警策略之后，只剩最后一块难啃的石头，也就是今天讨论的数据库状态监控。 Exchange 数据库监控的演变集团 Exchange 数据库的监控经历了 3 个主要阶段： 最初是根据一个数据库列表文档跑脚本，通过 Test-MAPIConnectivity 检测数据库是否挂载，发送短信来告警。手工维护难免造成遗漏； 在经历了一个由于主备关系问题导致的集群崩溃事故之后，第二阶段的脚本使用 Get-MailboxDatabaseCopyStatus 来侦测数据库状态。另外可以自动的检测新加入的数据库； 但是数据库大小、人数、可用空间、主备关系这些东西没有直观的展示出来，于是进化到第三阶段，Zabbix 统一监控展示。 Zabbix 监控数据库思路目前所有的邮件服务器均已加入 Zabbix 做基础的硬件信息和服务、性能指标的监控。 使用 Zabbix 发现服务，通过一个 Powershell 脚本( Get-ExchangeDBDiscovery.ps1 )获取到每台服务器中的数据库信息，写入到服务器监控项中；为了减少抓取延迟，服务器本地每分钟会执行另一个 Powershell 脚本(Get-ExchangeDBStatus.ps1)，将所需要的数据库信息全部写入到脚本执行目录中的子目录 DBStatus 中；Zabbix 中的服务器，通过执行自定义命令从 CSV 文件中获取字段值，完成数据收集；如果数据库被卸载，则触发一级告警，直接电话通知我。若数据库同步状态异常，则触发二级告警，使用集团内部 IM 软件告诉我。 困难由于我的目标是做成全自动监控，而且数据库数量实在太多（集团接近 50000 人，分了 300 多个数据库）所以其中比较困难的点在于： 由于数据库分 Active 和 Passive，会在两台成对的服务器中各生成一个监控项，降低 Zabbix 执行效率，所以必须分开 由于 Active 和 Passive 会切换，如果监控项也跟着切换，会造成临时性数据获取失败 主备关系切换同样会导致挂载信息和复制信息的混乱 解决方案对于第一点和第二点，通过 Get-MailboxDatabaseCopyStatus 中数据库挂载优先级字段 ActivationPreference 将主备关系固定，去掉优先级是 2 的数据库。 对于第三点，使用了一个过滤方法 Get-MailboxDatabaseCopyStatus | Where {$_.status -like &quot;*ount*&quot;} ,ount 是 Active 的数据库可能的四种状态：Mounted、Dismounted、Mounting、Dismounting。 源码使用方法源码我已经放在了 GitHub，使用方法： 将 Get-ExchangeDBStatus.ps1 加入计划任务，每分钟执行一次 修改 Exchange 服务器的 Zabbix Agent 服务配置，可以参考 zabbix_agentd.conf Zabbix 中导入模版 ZabbixTemplate_Exchange Server 2016 Database Monitor.xml 将源码中的另外两个脚本 Get-ExchangeDBDiscovery.ps1、Get-ExchangeDB.ps1 放到与 zabbix_agentd.conf 同级或自定义目录中（放置在自定义目录需要修改 zabbix_agentd.conf 中的脚本路径） 如果是较大型的企业或数据库服务器较多，推荐将三个 powershell 脚本置于 UNC 路径中，Zabbix 是支持的，这样便于部署配置。 后记其实在早期曾经尝试过在一台机器上进行所有数据库的收集，这样会让整个方案建立起来快速和方便得多，只需要执行一次脚本，也不需要那么多的判断。但是对于 50 个以上数据库规模的架构，这个方法不适用了，一方面脚本执行效率堪忧，完整执行一次可能要 5 分钟以上，这对于我们 99.995% 的 SLA 要求显然是不能够满足需求的；其次 Zabbix 对于单个主机的监控项收集效率也不够，实测起来可能会产生最多 15 分钟的延迟告警，这样是无法接受的。 如果是小型公司，数据库数量使用单一脚本做全部数据库收集显然是没问题的","link":"/2019/03/17/moniting-exchange-server-with-zabbix/"},{"title":"Nginx禁止其他域名访问","text":"今天看到一则来自@kyaky的评论 让我也意识到了这个问题: 如果别人把我的IP地址绑到他的域名上, 那我就是在为别人搭博客了. 我本以为Nginx配置中的server_name选项可以进行阻挡, 看来不实践真的是不能轻易下结论. 今天就回顾一下这个问题的所在, 及解决方案. DNS解析场景是这样: 域名 类型 IP A.com A a.b.c.d B.com A a.b.c.d 现在我的网站配置的默认服务是A.com.conf: server { listen 80 default; server_name A.com; } 当初我为了避免直接拿IP访问站点, 做了IP的过滤: if ( $host ~* &quot;\\d+\\.\\d+\\.\\d+\\.\\d+&quot; ) { return 403; } 在这种情况下, 访问A.com会找到a.b.c.d这个IP, Nginx解析得知匹配到了A.com.conf, 于是进入document root目录进行服务. 当访问B.com时, DNS也会找到a.b.c.d这个IP, Nginx解析得知没有匹配, 于是也会进入A.com.conf的document root, 继续提供服务 – 因为是default. 当时忽略了域名绑定IP的问题, 那么我再加一条if{}判断, 不符合archean.me的都返回403, 即可解决这个问题了. 但这样做显然不够优雅, 不仅陡然增加了Nginx配置文件的复杂度, 同时如果有多个域名绑定这台机器, 需要修改的地方就太多了. 所以查阅Nginx官方资料, 果然有相应的解决办法 原来我没有做默认服务default.conf的配置, A Default &quot;Catch All&quot; Server Block server { listen 80 default_server; server_name _; # This is just an invalid value which will never trigger on a real hostname. return 403; } 关键在于server_name _;, 这是指无效域名, 无论是IP地址, 错误的域名拼写或是未在其他虚拟主机配置中提到的域名, 均会触发return 403;, 显示页面不存在. 于是略加改进, 去掉其他虚拟主机中的default字段; 把default.conf中的return 403;改为重定向到我们期望的默认首页, 即可解决我们的问题. server { listen 80 default_server; server_name _; rewrite ^ http://archean.me$request_uri?; }","link":"/2013/10/24/nginx-deny-other-domain-access/"},{"title":"使用Squid与Stunnel构建安全的http代理服务器","text":"使用Squid在服务器端打开一个http 验证的代理端口, 同时用服务器上的Stunnel进行转发, 与客户端的Stunnel通过SSL链接, 达到代理的目的. 也可以使用客户端的Stunnel与Squid通过SSL直接相连. 本方法使用前者. 1. 服务器端配置服务器环境: 12345678# lsb_release -aLSB Version: :core-4.0-ia32:core-4.0-noarch:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0- ia32:printing-4.0-noarchDistributor ID: CentOSDescription: CentOS Linux release 6.0 (Final)Release: 6.0Codename: Final # uname -aLinux jb1.archean.me 2.6.32-71.el6.i686 #1 SMP Fri Nov 12 04:17:17 GMT 2010 i686 i686 i386 GNU/Linux 1.1 安装Squid下载squid 3.2.8 1# wget http://www.squid-cache.org/Versions/v3/3.2/squid-3.2.8.tar.gz 可以使用CentOS的Yum安装工具, 不过我更喜欢编译安装(提前准备好编译环境, Gcc, openssl等): 12345# tar zxvf squid-3.2.8.tar.gz# cd squid-3.2.8# ./configure --prefix=/usr/local --enable-basic-auth-helpers=NCSA# make# make install 1.2 配置Squid这个拓扑结构只需要Squid做简单的http代理, 所以无需SSL. squid的配置文件在/usr/local/etc/squid.conf 备份之后, 将其按下面修改, 为防止被别的机器滥用, 只监听127.0.0.1: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748###/usr/local/etc/squid.conf###2013-1-27 19:56 v0.0.1 for squid 3.2.6#Xu Zhang &lt;zephyr422@gmail.com&gt;visible_hostname Archean.mecache_mgr zephyr422@gmail.comhttp_port 127.0.0.1:3177icp_port 0cache_mem 256 MBdns_nameservers 8.8.8.8 8.8.4.4coredump_dir /usr/local/var/cache/squidaccess_log /usr/local/var/logs/squid_access.logcache_log /usr/local/var/logs/squid_cache.logauth_param basic program /usr/local/libexec/basic_ncsa_auth /usr/local/etc/squid.passwd #使用 HTTP 基本验证auth_param basic children 5auth_param basic realm Archean's GFW Breaker Proxy, to forward, please input &quot;Username/Password&quot;.auth_param basic credentialsttl 7 daysauth_param basic casesensitive offacl password proxy_auth REQUIREDacl localnet src 10.0.0.0/8 # RFC1918 possible internal networkacl localnet src 172.16.0.0/12 # RFC1918 possible internal networkacl localnet src 192.168.0.0/16 # RFC1918 possible internal networkacl localnet src fc00::/7 # RFC 4193 local private network rangeacl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machinesacl SSL_ports port 443acl Safe_ports port 80 # httpacl Safe_ports port 21 # ftpacl Safe_ports port 443 # httpsacl Safe_ports port 70 # gopheracl Safe_ports port 210 # waisacl Safe_ports port 1025-65535 # unregistered portsacl Safe_ports port 280 # http-mgmtacl Safe_ports port 488 # gss-httpacl Safe_ports port 591 # filemakeracl Safe_ports port 777 # multiling httpacl CONNECT method CONNECThttp_access allow passwordhttp_access deny !Safe_portshttp_access deny CONNECT !SSL_portshttp_access allow localnethttp_access allow localhosthttp_access deny all 检查配置有没有问题: 1# squid -k check 或者 1# squid -k parse 生成密码文件(-c), 创建账户. (如果没有htpasswd需要安装 httpd, 略) 123# htpasswd -c /usr/local/etc/squid.passwd archeannew password... 初始化cache目录 1# squid -z 一旦你已经初始化cache目录，就可以在终端窗口里运行squid，将日志记录到标准错误。这样，就能轻易的定位任何错误或问题，并且确认squid是否成功启动。使用-N选项来保持squid在前台运行，-d1选项在标准错误里显示1级别的调试信息。 1# squid -N -d1 启动squid: 1# squid 检查是否启动成功, ps -ef | grep squid 或 lsof -i:3177 模拟测试客户端连接: 1# squidclient -p 3177 http://www.squid-cache.org/ 如期返回了html信息, 说明Squid已成功启动. 1.3 安装Stunnel下载稳定版Stunnel 1# wget https://www.stunnel.org/downloads/stunnel-4.56.tar.gz 创建Stunnel用户: 12# /usr/sbin/groupadd -g 122 stunnel# /usr/sbin/useradd -c stunnel -d /nonexistent -m -g 122 -u 122 stunnel 安装: 12345# tar zxvf stunnel-4.56.tar.gz# cd stunnel-4.56# ./configure --prefix=/usr/local# make# makeinstall 安装过程通常会创建自签名证书, 会放到/usr/local/etc/stunnel/stunnel.pem可以直接使用(有效期一年). 使用下面的命令检查证书详细内容: 12345678# openssl x509 -subject -dates -fingerprint -in stunnel.pem subject= /C=CN/ST=Beijing/L=Beijing/O=Archean Inc/OU=Archean Inc/CN=archean.menotBefore=Apr 20 02:05:24 2013 GMTnotAfter=Apr 20 02:05:24 2014 GMTSHA1 Fingerprint=87:F8:6E:05:B8:9C:BC:A1:EA:15:B7:C9:B4:B2:75:FF:8A:CA:C5:FA-----BEGIN CERTIFICATE-----xxx-----END CERTIFICATE----- 给证书生成 Diffie-Hellman 部分 1# openssl gendh 512&gt;&gt; stunnel.pem 这在4.x版本的stunnel上好像是必须的. 如果想要自己生成证书, 命令如下: 1# openssl req -new -x509 -days 365 -nodes -config openssl.cnf -out stunnel.pem -keyout stunnel.pem 1.4 配置Stunnel在/usr/local/etc/stunnel/下创建stunnel.conf, 写入如下配置: 1234567891011121314151617181920212223242526272829303132cert = /usr/local/etc/stunnel/stunnel.pemCAfile = /usr/local/etc/stunnel/stunnel.pemsocket = l:TCP_NODELAY=1socket = r:TCP_NODELAY=1;;;chroot = /var/run/stunnelpid = /tmp/stunnel.pidverify = 3;;; CApath = certs;;; CRLpath = crls;;; CRLfile = crls.pemsetuid = stunnelsetgid = stunnel;;; client=yescompression = zlib;;; taskbar = nodelay = no;;; failover = rr;;; failover = priosslVersion = TLSv1fips=nodebug = 7syslog = nooutput = stunnel.log[sproxy]accept = 34567connect = 127.0.0.1:3177 此时便可启动stunnel: 1# stunnel 检查是否运行: 12# ps -ef | grep stunnel# lsof -i:34567 1.5 将Squid和Stunnel加入开机启动项略 2. 客户端配置2.1 linux客户端使用stunnel与服务器进行安全连接安装Stunnel 与服务器完全相同, 略. 2.2 配置客户端Stunnel将服务器生成的证书传到客户端中: 12# cd /usr/local/etc/stunnel# scp root@jb1.archean.me:/usr/local/etc/stunnel/stunnel.pem ./ 创建配置文件 1# vim stunnel.conf 内容如下: 12345678910111213141516id = /tmp/stunnel.pidcert = /usr/local/etc/stunnel/stunnel.pemsocket = l:TCP_NODELAY=1socket = r:TCP_NODELAY=1verify = 2CAfile = /usr/local/etc/stunnel/stunnel.pemclient=yescompression = zlibciphers = AES256-SHAdelay = nofailover = priosslVersion = TLSv1fips = no[sproxy]accept = 0.0.0.0:7071connect = jb1.archean.me:34567 其中accept是本地代理监听地址, 如不对外提供服务则改为accept = 127.0.0.1:7071 启动stunnel: 1# /usr/local/bin/stunnel 至此, 配置完全结束, 可以通过使用Client.IP.Address:7071代理上网 2.3 结语参考资料: Squid权威指南","link":"/2013/10/16/safe-http-proxy-over-squid-and-stunnel/"},{"title":"Secure Proxy Service","text":"0. 免责声明1此服务仅用于研究互联网技术, 使用者访问了任何网站或发表了任何言论, 不代表本人支持其观点. 1. 描述这是一个”加密的” socks 代理服务, 使用者能够安全的访问世界上任何一个网站, 在本地网络状况正常的情况下, 可以: 流畅的播放 YouTube 720P 视频, 基本流畅的播放 1080P 视频: 访问 Facebook, Twitter, Instagram 等网站. 为一些 app 提供访问支持. 2. 如何使用我会将服务 IP/端口(全局代理方式)和 PAC URL(Proxy Auto Config 方式) 以邮件形式发送到你的私人邮箱中, 获取到之后, 分别按照如下说明使用: 全局代理在 IE 的 Lan 代理设置, Chrome 的 SwitchyOmega 插件, 或 mac 的网络代理配置中填写服务 IP 及端口, 正确操作后, 所有 http 服务都将走代理访问, 会导致部分网站访问速度变慢. Proxy Auto Config 文件在 IE 的 Lan 代理设置, Chrome 的 SwitchyOmega 插件, 或 mac 的网络代理配置中填写 pac 文件位置, 正确操作后, pac 白名单中的 http 服务将走代理访问, 推荐方式. 3. 现存问题网络状况众所周知中国几个跨洋光缆不是那么稳定, 代理服务有时会出现网络波动及不可用(非常少, 4年中大概总计1天)的问题, 我这里有几个不同的国外服务器, 在出现异常情况时会及时切换. 端口被复用提供给你的服务端口或 PAC 文件是完全开放的, 意味着一旦泄露, 别人可以直接拿来用, 所以强烈建议不要与其他人共享. 关于此事, 我正研究不影响用户体验的验证方式, 后续将会改版. 安全问题虽然我保证不会这么做, 且也没什么必要, 但我是有技术手段监听你所有流量的, 请在确保不会提交隐私信息的情况下使用. 4. 费用费用是10元/月, 或100元/年, 接受 Alipay 或 PayPal 转账, 帐号均为 zephyr422@gmail.com. 如果服务由于不可抗力中断, 我会退给你剩余时间的费用. 5. 一些解释加密的: 在国内-&gt;国外这个阶段的所有流量会经过我的加密, 不会遭遇政府的窃取. 访问速度: 与使用人数, 本地网络状况及国际线路有关, 我会尽可能确保稳定.","link":"/2014/01/23/secure-proxy-service/"}],"tags":[],"categories":[{"name":"Apple","slug":"Apple","link":"/categories/Apple/"},{"name":"Moniting","slug":"Moniting","link":"/categories/Moniting/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"Experience","slug":"Apple/Experience","link":"/categories/Apple/Experience/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"F5","slug":"Moniting/F5","link":"/categories/Moniting/F5/"},{"name":"Exchange","slug":"Moniting/Exchange","link":"/categories/Moniting/Exchange/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"web","slug":"web","link":"/categories/web/"},{"name":"iptables","slug":"linux/iptables","link":"/categories/linux/iptables/"},{"name":"Web","slug":"Web","link":"/categories/Web/"},{"name":"aws","slug":"linux/aws","link":"/categories/linux/aws/"},{"name":"web","slug":"linux/web","link":"/categories/linux/web/"},{"name":"gcp","slug":"linux/aws/gcp","link":"/categories/linux/aws/gcp/"}]}